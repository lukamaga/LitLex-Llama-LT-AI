from unsloth import FastLanguageModel
import torch

MODEL_NAME = "lukashm/LitLex-Llama-LT-v1"

def run_test():
    print(f"â³ Loading model from: {MODEL_NAME}...")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_NAME,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )

    FastLanguageModel.for_inference(model)

    alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{}

### Response:
{}"""

    questions = [
        "Kokia bauda gresia uÅ¾ greiÄio virÅ¡ijimÄ… daugiau kaip 50 km/h?",
        "KÄ… daryti, jei kaimynai triukÅ¡mauja naktÄ¯? Kokia bauda?",
        "Kokia bauda uÅ¾ nelegalÅ³ darbÄ…?",
        "Kokia bauda uÅ¾ triukÅ¡mavimÄ… vakaro metu?"
    ]

    print("\nâš–ï¸ LITLEX AI TEISINÄ– KONSULTACIJA âš–ï¸\n")

    for q in questions:
        inputs = tokenizer(
            [
                alpaca_prompt.format(
                    q,
                    "",
                )
            ], return_tensors="pt").to("cuda")

        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id
        )

        response = tokenizer.batch_decode(outputs)[0]

        clean_response = response.split("### Response:\n")[1].replace("<|end_of_text|>", "").strip()

        print(f"ğŸ‘¤ Klausimas: {q}")
        print(f"ğŸ¤– Atsakymas: {clean_response}")
        print("-" * 50)


if __name__ == "__main__":
    run_test()
